---
alwaysApply: false
---
Use OpenAI Responses API (/v1/responses), not chat/completions.
Always include:
- model from env, temperature 0
- prompt: { id: "pmpt_68ae03fd6e6481908a8939a9e9272e130cf3d534ebfbb3d9" }
- tools: our capability functions (get_available_triggers, get_available_workflow_steps, get_reference_data, validate_automation_data, commit_workflow)
- tool_choice: "auto"
- response_format: strict JSON schema `assistant_reply` (display_text + ui_suggestions[ {id,label,payload,variant?,tool_call?} ])

Create/maintain:
- server LLM client that runs a tool loop and returns { display_text, ui_suggestions }
- React UI that renders buttons from ui_suggestions and sends payload (or executes tool_call)
Follow grounding rules: only suggest values returned by tools; unsupported → say so and suggest nearest supported.Use OpenAI Responses API (/v1/responses), not chat/completions.
Always include:
- model from env, temperature 0
- prompt: { id: "pmpt_68ae03fd6e6481908a8939a9e9272e130cf3d534ebfbb3d9" }
- tools: our capability functions (get_available_triggers, get_available_workflow_steps, get_reference_data, validate_automation_data, commit_workflow)
- tool_choice: "auto"
- response_format: strict JSON schema `assistant_reply` (display_text + ui_suggestions[ {id,label,payload,variant?,tool_call?} ])

Create/maintain:
- server LLM client that runs a tool loop and returns { display_text, ui_suggestions }
- React UI that renders buttons from ui_suggestions and sends payload (or executes tool_call)
Follow grounding rules: only suggest values returned by tools; unsupported → say so and suggest nearest supported.